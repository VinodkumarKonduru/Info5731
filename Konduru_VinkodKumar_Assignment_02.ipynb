{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Wednesday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (40 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from G2 or Capterra.\n",
        "\n",
        "(4) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the Densho Digital Repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "#headers for connecting to amazon\n",
        "headers = {\n",
        "    'authority': 'www.amazon.com',\n",
        "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,/;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
        "    'accept-language': 'en-US,en;q=0.9,bn;q=0.8',\n",
        "    'sec-ch-ua': '\" Not A;Brand\";v=\"99\", \"Chromium\";v=\"102\", \"Google Chrome\";v=\"102\"',\n",
        "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36'\n",
        "}\n",
        "\n",
        "#url for apple charger to fetch reviews\n",
        "amazon_reviews_url = 'https://www.amazon.com/Apple-20W-USB-C-Power-Adapter/product-reviews/B08L5M9BTJ/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews'\n",
        "\n",
        "#took 2000 pages so that we get min of 1000 reviews for sure\n",
        "pages_of_reviews = 2000\n",
        "\n",
        "\n",
        "#this function just the url and takes html of all the pages\n",
        "def reviewsHtml(url, len_page):\n",
        "\n",
        "    bs = []\n",
        "\n",
        "    for page_no in range(1, pages_of_reviews + 1):\n",
        "\n",
        "        params = {\n",
        "            'ie': 'UTF8',\n",
        "            'reviewerType': 'all_reviews',\n",
        "            'filterByStar': 'critical',\n",
        "            'pageNumber': page_no,\n",
        "        }\n",
        "\n",
        "        response = requests.get(url, headers=headers)\n",
        "        s = BeautifulSoup(response.text, 'lxml')\n",
        "        bs.append(s)\n",
        "\n",
        "    return bs\n",
        "\n",
        "\n",
        "#we get reviews from this function, we use css classes to identify the review title, desc date etc.,\n",
        "def getReviews(data):\n",
        "\n",
        "    review_dict = []\n",
        "    hooks = data.select('div[data-hook=\"review\"]')\n",
        "\n",
        "    for hook in hooks:\n",
        "\n",
        "        try:\n",
        "            name = hook.select_one('[class=\"a-profile-name\"]').text.strip()\n",
        "        except Exception as e:\n",
        "            name = 'N/A'\n",
        "\n",
        "        try:\n",
        "            rating = hook.select_one('[data-hook=\"review-star-rating\"]').text.strip().split(' out')[0]\n",
        "        except Exception as e:\n",
        "            rating = 'N/A'\n",
        "\n",
        "        try:\n",
        "            title = hook.select_one('[data-hook=\"review-title\"]').text.strip()\n",
        "        except Exception as e:\n",
        "            title = 'N/A'\n",
        "\n",
        "        try:\n",
        "            datetime_str = hook.select_one('[data-hook=\"review-date\"]').text.strip().split(' on ')[-1]\n",
        "            date = datetime.strptime(datetime_str, '%B %d, %Y').strftime(\"%d/%m/%Y\")\n",
        "        except Exception as e:\n",
        "            date = 'N/A'\n",
        "\n",
        "        try:\n",
        "            description = hook.select_one('[data-hook=\"review-body\"]').text.strip()\n",
        "        except Exception as e:\n",
        "            description = 'N/A'\n",
        "\n",
        "        data_dict = {\n",
        "            'Name' : name,\n",
        "            'Rating' : rating,\n",
        "            'Title' : title,\n",
        "            'Date' : date,\n",
        "            'Desc' : description\n",
        "        }\n",
        "\n",
        "        review_dict.append(data_dict)\n",
        "\n",
        "    return review_dict\n",
        "\n",
        "\n",
        "block_data = reviewsHtml(amazon_reviews_url, pages_of_reviews)\n",
        "\n",
        "reviews = []\n",
        "\n",
        "#we append the data to reviews list\n",
        "for data in block_data:\n",
        "    review = getReviews(data)\n",
        "    reviews += review\n",
        "\n",
        "df_reviews = pd.DataFrame(reviews)\n",
        "\n",
        "print(df_reviews)\n",
        "\n",
        "#storing the reviews in a csv file\n",
        "df_reviews.to_csv('amazonreviews.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNveb7rh9f_3",
        "outputId": "d9e12ef0-e5e7-42ce-d77c-a8ec5c6476cd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                   Name Rating  \\\n",
            "0                                         SwamyLakamana    5.0   \n",
            "1                                       Reviews by Andy    5.0   \n",
            "2                                               Shirley    5.0   \n",
            "3                                          Justin Huber    5.0   \n",
            "4                                       Amazon Customer    5.0   \n",
            "...                                                 ...    ...   \n",
            "2195                                        M. Jennings    5.0   \n",
            "2196                                  Christopher Pitts    4.0   \n",
            "2197                                              Diana    5.0   \n",
            "2198                                              Marty    1.0   \n",
            "2199  NLineâ€¦.. Not at all what expected failed to wo...    5.0   \n",
            "\n",
            "                                                  Title        Date  \\\n",
            "0           5.0 out of 5 stars\\nAuthentic Apple Adapter  17/11/2023   \n",
            "1     5.0 out of 5 stars\\nPower in a Compact Package...  25/01/2024   \n",
            "2                 5.0 out of 5 stars\\nUSB power adapter  18/02/2024   \n",
            "3     5.0 out of 5 stars\\nGood quality charger, dece...  05/01/2024   \n",
            "4               5.0 out of 5 stars\\nGood charging block  20/02/2024   \n",
            "...                                                 ...         ...   \n",
            "2195            5.0 out of 5 stars\\nSturdy and portable  09/02/2024   \n",
            "2196           4.0 out of 5 stars\\nDoes what is should.  16/02/2024   \n",
            "2197                  5.0 out of 5 stars\\nDoes the job!  24/02/2024   \n",
            "2198  1.0 out of 5 stars\\nDamned if they do, damned ...  07/10/2023   \n",
            "2199              5.0 out of 5 stars\\nApple Always Good  26/02/2024   \n",
            "\n",
            "                                                   Desc  \n",
            "0     I've been using the authentic Apple 20W Charge...  \n",
            "1     The Apple 20W USB-C Power Adapter is a true po...  \n",
            "2     Power adapter works well and great valueSpeedy...  \n",
            "3     bought this just for the charger. NO Cords inc...  \n",
            "4                       Works with our iPad and iPhone.  \n",
            "...                                                 ...  \n",
            "2195  This is a fast charger that is convenient to c...  \n",
            "2196  Thereâ€™s not a lot to say about something like ...  \n",
            "2197                            Great and fast charger.  \n",
            "2198  I'm using my Apple device cable and this charg...  \n",
            "2199  They have customer buying habits down pat. Soo...  \n",
            "\n",
            "[2200 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (30 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5QX6bJjGWXY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab1158dc-c088-4060-cd95-55eedf530985"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "df_reviews = pd.read_csv('amazonreviews.csv')\n",
        "\n",
        "#we use regular expresiions for identifying the words and then find stop words using stopwords library\n",
        "# porterstemmer for stemming and wordnetlemmatizer for lemmetizing the sentence\n",
        "def clean_text(txt):\n",
        "    txt = re.sub(r'[^a-zA-Z\\s]', '', txt)\n",
        "    txt = re.sub(r'\\d+', '', txt)\n",
        "    txt = txt.lower()\n",
        "    stwords = set(stopwords.words('english'))\n",
        "    wds = txt.split()\n",
        "    fwds = [word for word in wds if word not in stwords]\n",
        "    txt = ' '.join(fwds)\n",
        "    ps = PorterStemmer()\n",
        "    stmwds = [ps.stem(word) for word in fwds]\n",
        "    txt = ' '.join(stmwds)\n",
        "    lmtze = WordNetLemmatizer()\n",
        "    lmt_wrds = [lmtze.lemmatize(word) for word in fwds]\n",
        "    txt = ' '.join(lmt_wrds)\n",
        "    return txt\n",
        "\n",
        "df_reviews[''] = df_reviews['Desc'].apply(clean_text)\n",
        "df_reviews.to_csv('cleaned_amazon_reviews.csv', index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (30 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "df_cleaned_reviews = pd.read_csv('cleaned_amazon_reviews.csv')\n",
        "\n",
        "def pos_tagging(text):\n",
        "    tkns = word_tokenize(text)\n",
        "    pos_tags = pos_tag(tkns)\n",
        "    pos_counts = {'Noun': 0, 'Verb': 0, 'Adjective': 0, 'Adverb': 0}\n",
        "\n",
        "    #this for loop identifies the pos of a word using pos)tag function\n",
        "    for word, pos in pos_tags:\n",
        "        if pos.startswith('N'):\n",
        "            pos_counts['Noun'] += 1\n",
        "        elif pos.startswith('V'):\n",
        "            pos_counts['Verb'] += 1\n",
        "        elif pos.startswith('J'):\n",
        "            pos_counts['Adjective'] += 1\n",
        "        elif pos.startswith('R'):\n",
        "            pos_counts['Adverb'] += 1\n",
        "    return pos_counts\n",
        "#initially\n",
        "total_pos_counts = {'Noun': 0, 'Verb': 0, 'Adjective': 0, 'Adverb': 0}\n",
        "\n",
        "#go through all the words in the reviews and identify its pos\n",
        "for index, row in df_cleaned_reviews.iterrows():\n",
        "    review_text = row['Desc']\n",
        "    # Calculate POS counts for the current review\n",
        "    pos_counts = pos_tagging(review_text)\n",
        "    # Accumulate counts for each POS category\n",
        "    for pos_category, count in pos_counts.items():\n",
        "        total_pos_counts[pos_category] += count\n",
        "\n",
        "# Print total counts of POS categories\n",
        "print(\"Total Counts of POS Categories:\")\n",
        "print(total_pos_counts)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xb-B5g93bif",
        "outputId": "293eb58c-82c1-4b4f-ceb6-9b1b4a6f03a2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Counts of POS Categories:\n",
            "{'Noun': 147620, 'Verb': 91960, 'Adjective': 47080, 'Adverb': 29040}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tree import Tree\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "df_cleaned_reviews = pd.read_csv('cleaned_amazon_reviews.csv')\n",
        "\n",
        "#Constituency parsing aims to identify the syntactic structure of sentences\n",
        "#based on the rules of a formal grammar.\n",
        "def cptree(text):\n",
        "    tkns = word_tokenize(text)\n",
        "    tagged_tokens = nltk.pos_tag(tkns)\n",
        "    grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "    cp = nltk.RegexpParser(grammar)\n",
        "    parsed_sentence = cp.parse(tagged_tokens)\n",
        "    print(\"CPT:\")\n",
        "    parsed_sentence.pretty_print()\n",
        "\n",
        "#shows dependencies between the words\n",
        "def dptree(text):\n",
        "    tkns = word_tokenize(text)\n",
        "    pos_tags = nltk.pos_tag(tkns)\n",
        "    grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "    cp = nltk.RegexpParser(grammar)\n",
        "    parsed_sentence = cp.parse(pos_tags)\n",
        "    print(\"DPT:\")\n",
        "    print(parsed_sentence)\n",
        "\n",
        "#taking first review as an example to show\n",
        "example_review = df_cleaned_reviews.iloc[0]['Desc']\n",
        "\n",
        "cptree(example_review)\n",
        "dptree(example_review)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0UwqzjK4N6d",
        "outputId": "44883536-a24f-4fcf-dd06-0c9fe00a4781"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Constituency Parsing Tree:\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           S                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
            "   ________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________|_________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________                    \n",
            "  |      |       |         |       |         |           |       |         |        |      |     |    |      |      |         |         |           |           |    |     |       |        |       |        |          |         |       |        |          |      |     |    |         |          |       |      |       |      |     |      |        |       |         |             |            |         |        |      |       |      |    |      |          |            |       |      |        |       |      |      |       |      |       |          |       |      |         |      |    |         |             |            |        |      |        |     |         |        |      |        |          |          |        |      |        |         |         |        |         |           |        |            |               |         |       |       |       |      |     |         |            |            |        |      |        |        |          |          |        |       |        |           |         |       |      |         |        |    |      |           |            |         |         |            |         |    |    |       |         |       |      |       |       |      |     |       |      |     |      |      |         |         |        |           |          |        |           |       |     |         |        |       |       |       |       |       |        |       |    |      |      |       |       |      |        |       |       |        |         |         |        |           |        |    |     |        |       |         |        |          |          |       |      |     |   |    |      |         |        |    |      |       |         |        |     |        |        |         |            |            |         |       |         |          |       |    |     |      |             |         |        NP                    NP               NP       NP      NP        NP          NP                NP                      NP                      NP                 NP             NP      NP             NP            NP        NP             NP                    NP               NP               NP               NP        NP       NP       NP            NP                 NP                      NP                   NP                          NP                NP                      NP            NP                NP                           NP              NP                NP                       NP               NP         NP           NP              NP                       NP                 \n",
            "  |      |       |         |       |         |           |       |         |        |      |     |    |      |      |         |         |           |           |    |     |       |        |       |        |          |         |       |        |          |      |     |    |         |          |       |      |       |      |     |      |        |       |         |             |            |         |        |      |       |      |    |      |          |            |       |      |        |       |      |      |       |      |       |          |       |      |         |      |    |         |             |            |        |      |        |     |         |        |      |        |          |          |        |      |        |         |         |        |         |           |        |            |               |         |       |       |       |      |     |         |            |            |        |      |        |        |          |          |        |       |        |           |         |       |      |         |        |    |      |           |            |         |         |            |         |    |    |       |         |       |      |       |       |      |     |       |      |     |      |      |         |         |        |           |          |        |           |       |     |         |        |       |       |       |       |       |        |       |    |      |      |       |       |      |        |       |       |        |         |         |        |           |        |    |     |        |       |         |        |          |          |       |      |     |   |    |      |         |        |    |      |       |         |        |     |        |        |         |            |            |         |       |         |          |       |    |     |      |             |         |    ____|_____          ______|______          |        |       |         |           |            _____|______           ______|_____           _______|______        ____|______        |       |        ______|_____        |         |         _____|______          _____|____            |           _____|______          |         |        |        |        _____|_____        _____|________               |           _________|_____________              |           ______|________               |             |        _________|_____________               |           ____|____        _____|________           _____|______          |          |            |          _____|______          ________|___________        \n",
            "I/PRP 've/VBP been/VBN using/VBG the/DT authentic/JJ Apple/NNP 20W/CD Charger/NNP for/IN now/RB ,/, and/CC it/PRP 's/VBZ exceeded/JJ my/PRP$ expectations/NNS in/IN ./. When/WRB it/PRP comes/VBZ to/TO charging/VBG my/PRP$ devices/NNS ,/, especially/RB my/PRP$ and/CC ,/, I/PRP prioritize/VBP not/RB just/RB but/CC also/RB and/CC ./. ticks/VBZ all/PDT those/DT boxes/NNS effortlessly.The/ that/WDT stands/VBZ out/RP is/VBZ its/PRP$ ./. It/PRP 's/VBZ incredibly/RB portable/JJ ,/, making/VBG it/PRP easy/JJ to/TO slip/VB into/IN or/CC without/IN adding/VBG ./. Despite/IN its/PRP$ ,/, it/PRP delivers/VBZ remarkable/JJ charging/VBG ./. Whether/CC I/PRP 'm/VBP topping/VBG up/RP my/PRP$ quickly/RB before/IN heading/VBG out/RP or/CC charging/VBG it/PRP overnight/JJ ,/, consistently/RB gets/VBZ done/VBN swiftly.What/WDT reassures/VBZ me/PRP most/RBS is/VBZ its/PRP$ ./. With/IN numerous/JJ knockoffs/NNS flooding/VBG ,/, having/VBG the/DT genuine/JJ Apple/NNP ensures/VBZ not/RB only/RB faster/RBR charging/VBG but/CC also/RB of/IN regarding/VBG ./. I/PRP 've/VBP encountered/VBN issues/NNS with/IN non-Apple/JJ chargers/NNS in/IN ,/, and/CC they/PRP simply/RB do/VBP n't/RB match/VB and/CC of/IN is/VBZ its/PRP$ ./. This/DT 20W/CD is/VBZ versatile/JJ and/CC works/VBZ seamlessly/RB with/IN various/JJ devices/NNS ,/, from/IN iPhones/NNS to/TO iPads/NNS and/CC even/RB some/DT Mac/NNP models/NNS ./. It/PRP 's/VBZ for/IN most/JJS of/IN my/PRP$ Apple/NNP it/PRP might/MD seem/VB pricier/JJR than/IN generic/JJ chargers/NNS ,/, in/IN the/DT Apple/NNP 20W/CD Charger/NNP is/VBZ definitely/RB worth/JJ it/PRP for/IN ,/, ,/, and/CC it/PRP provides/VBZ ./. If/IN you/PRP 're/VBP looking/VBG for/IN a/DT reliable/JJ and/CC efficient/JJ charging/VBG tailored/VBD to/TO your/PRP$ Apple/NNP devices/NNS ,/, is/VBZ ./. Highly/NNP recommended/VBD !/. a/DT     while/NN every/DT     aspect/NN iPhone/NN iPad/NN speed/NN safety/NN reliability/NN This/DT     charger/NN first/JJ     thing/NN compact/JJ     design/NN a/DT     pocket/NN bag/NN bulk/NN small/JJ     size/NN speed/NN device/NN this/DT     charger/NN the/DT     job/NN authenticity/NN the/DT     market/NN product/NN peace/NN mind/NN safety/NN the/DT     past/NN the/DT     reliability/NN performance/NN the/DT authentic/JJ one.Another/NN advantage/NN worth/JJ     mentioning/NN compatibility/NN charger/NN a/DT one-size-fits- solution/NN gadgets.Although/ a/DT     bit/NN the/DT     investment/NN the/DT     quality/NN speed/NN assurance/NN solution/NN this/DT     charger/NN an/DT absolute/JJ must-have/NN\n",
            "                                                                                                                                                                                                                                                                                                                                                                                        VBP                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              all/JJ                         NN                                                                                                                                                              \n",
            "\n",
            "Dependency Parsing Tree:\n",
            "(S\n",
            "  I/PRP\n",
            "  've/VBP\n",
            "  been/VBN\n",
            "  using/VBG\n",
            "  the/DT\n",
            "  authentic/JJ\n",
            "  Apple/NNP\n",
            "  20W/CD\n",
            "  Charger/NNP\n",
            "  for/IN\n",
            "  (NP a/DT while/NN)\n",
            "  now/RB\n",
            "  ,/,\n",
            "  and/CC\n",
            "  it/PRP\n",
            "  's/VBZ\n",
            "  exceeded/JJ\n",
            "  my/PRP$\n",
            "  expectations/NNS\n",
            "  in/IN\n",
            "  (NP every/DT aspect/NN)\n",
            "  ./.\n",
            "  When/WRB\n",
            "  it/PRP\n",
            "  comes/VBZ\n",
            "  to/TO\n",
            "  charging/VBG\n",
            "  my/PRP$\n",
            "  devices/NNS\n",
            "  ,/,\n",
            "  especially/RB\n",
            "  my/PRP$\n",
            "  (NP iPhone/NN)\n",
            "  and/CC\n",
            "  (NP iPad/NN)\n",
            "  ,/,\n",
            "  I/PRP\n",
            "  prioritize/VBP\n",
            "  not/RB\n",
            "  just/RB\n",
            "  (NP speed/NN)\n",
            "  but/CC\n",
            "  also/RB\n",
            "  (NP safety/NN)\n",
            "  and/CC\n",
            "  (NP reliability/NN)\n",
            "  ./.\n",
            "  (NP This/DT charger/NN)\n",
            "  ticks/VBZ\n",
            "  all/PDT\n",
            "  those/DT\n",
            "  boxes/NNS\n",
            "  effortlessly.The/VBP\n",
            "  (NP first/JJ thing/NN)\n",
            "  that/WDT\n",
            "  stands/VBZ\n",
            "  out/RP\n",
            "  is/VBZ\n",
            "  its/PRP$\n",
            "  (NP compact/JJ design/NN)\n",
            "  ./.\n",
            "  It/PRP\n",
            "  's/VBZ\n",
            "  incredibly/RB\n",
            "  portable/JJ\n",
            "  ,/,\n",
            "  making/VBG\n",
            "  it/PRP\n",
            "  easy/JJ\n",
            "  to/TO\n",
            "  slip/VB\n",
            "  into/IN\n",
            "  (NP a/DT pocket/NN)\n",
            "  or/CC\n",
            "  (NP bag/NN)\n",
            "  without/IN\n",
            "  adding/VBG\n",
            "  (NP bulk/NN)\n",
            "  ./.\n",
            "  Despite/IN\n",
            "  its/PRP$\n",
            "  (NP small/JJ size/NN)\n",
            "  ,/,\n",
            "  it/PRP\n",
            "  delivers/VBZ\n",
            "  remarkable/JJ\n",
            "  charging/VBG\n",
            "  (NP speed/NN)\n",
            "  ./.\n",
            "  Whether/CC\n",
            "  I/PRP\n",
            "  'm/VBP\n",
            "  topping/VBG\n",
            "  up/RP\n",
            "  my/PRP$\n",
            "  (NP device/NN)\n",
            "  quickly/RB\n",
            "  before/IN\n",
            "  heading/VBG\n",
            "  out/RP\n",
            "  or/CC\n",
            "  charging/VBG\n",
            "  it/PRP\n",
            "  overnight/JJ\n",
            "  ,/,\n",
            "  (NP this/DT charger/NN)\n",
            "  consistently/RB\n",
            "  gets/VBZ\n",
            "  (NP the/DT job/NN)\n",
            "  done/VBN\n",
            "  swiftly.What/WDT\n",
            "  reassures/VBZ\n",
            "  me/PRP\n",
            "  most/RBS\n",
            "  is/VBZ\n",
            "  its/PRP$\n",
            "  (NP authenticity/NN)\n",
            "  ./.\n",
            "  With/IN\n",
            "  numerous/JJ\n",
            "  knockoffs/NNS\n",
            "  flooding/VBG\n",
            "  (NP the/DT market/NN)\n",
            "  ,/,\n",
            "  having/VBG\n",
            "  the/DT\n",
            "  genuine/JJ\n",
            "  Apple/NNP\n",
            "  (NP product/NN)\n",
            "  ensures/VBZ\n",
            "  not/RB\n",
            "  only/RB\n",
            "  faster/RBR\n",
            "  charging/VBG\n",
            "  but/CC\n",
            "  also/RB\n",
            "  (NP peace/NN)\n",
            "  of/IN\n",
            "  (NP mind/NN)\n",
            "  regarding/VBG\n",
            "  (NP safety/NN)\n",
            "  ./.\n",
            "  I/PRP\n",
            "  've/VBP\n",
            "  encountered/VBN\n",
            "  issues/NNS\n",
            "  with/IN\n",
            "  non-Apple/JJ\n",
            "  chargers/NNS\n",
            "  in/IN\n",
            "  (NP the/DT past/NN)\n",
            "  ,/,\n",
            "  and/CC\n",
            "  they/PRP\n",
            "  simply/RB\n",
            "  do/VBP\n",
            "  n't/RB\n",
            "  match/VB\n",
            "  (NP the/DT reliability/NN)\n",
            "  and/CC\n",
            "  (NP performance/NN)\n",
            "  of/IN\n",
            "  (NP the/DT authentic/JJ one.Another/NN)\n",
            "  (NP advantage/NN)\n",
            "  (NP worth/JJ mentioning/NN)\n",
            "  is/VBZ\n",
            "  its/PRP$\n",
            "  (NP compatibility/NN)\n",
            "  ./.\n",
            "  This/DT\n",
            "  20W/CD\n",
            "  (NP charger/NN)\n",
            "  is/VBZ\n",
            "  versatile/JJ\n",
            "  and/CC\n",
            "  works/VBZ\n",
            "  seamlessly/RB\n",
            "  with/IN\n",
            "  various/JJ\n",
            "  devices/NNS\n",
            "  ,/,\n",
            "  from/IN\n",
            "  iPhones/NNS\n",
            "  to/TO\n",
            "  iPads/NNS\n",
            "  and/CC\n",
            "  even/RB\n",
            "  some/DT\n",
            "  Mac/NNP\n",
            "  models/NNS\n",
            "  ./.\n",
            "  It/PRP\n",
            "  's/VBZ\n",
            "  (NP a/DT one-size-fits-all/JJ solution/NN)\n",
            "  for/IN\n",
            "  most/JJS\n",
            "  of/IN\n",
            "  my/PRP$\n",
            "  Apple/NNP\n",
            "  (NP gadgets.Although/NN)\n",
            "  it/PRP\n",
            "  might/MD\n",
            "  seem/VB\n",
            "  (NP a/DT bit/NN)\n",
            "  pricier/JJR\n",
            "  than/IN\n",
            "  generic/JJ\n",
            "  chargers/NNS\n",
            "  ,/,\n",
            "  (NP the/DT investment/NN)\n",
            "  in/IN\n",
            "  the/DT\n",
            "  Apple/NNP\n",
            "  20W/CD\n",
            "  Charger/NNP\n",
            "  is/VBZ\n",
            "  definitely/RB\n",
            "  worth/JJ\n",
            "  it/PRP\n",
            "  for/IN\n",
            "  (NP the/DT quality/NN)\n",
            "  ,/,\n",
            "  (NP speed/NN)\n",
            "  ,/,\n",
            "  and/CC\n",
            "  (NP assurance/NN)\n",
            "  it/PRP\n",
            "  provides/VBZ\n",
            "  ./.\n",
            "  If/IN\n",
            "  you/PRP\n",
            "  're/VBP\n",
            "  looking/VBG\n",
            "  for/IN\n",
            "  a/DT\n",
            "  reliable/JJ\n",
            "  and/CC\n",
            "  efficient/JJ\n",
            "  charging/VBG\n",
            "  (NP solution/NN)\n",
            "  tailored/VBD\n",
            "  to/TO\n",
            "  your/PRP$\n",
            "  Apple/NNP\n",
            "  devices/NNS\n",
            "  ,/,\n",
            "  (NP this/DT charger/NN)\n",
            "  is/VBZ\n",
            "  (NP an/DT absolute/JJ must-have/NN)\n",
            "  ./.\n",
            "  Highly/NNP\n",
            "  recommended/VBD\n",
            "  !/.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "df_cleaned_reviews = pd.read_csv('cleaned_amazon_reviews.csv')\n",
        "\n",
        "# Named Entity Recognition by splitting words and identifying pos\n",
        "def named_entity_recognition(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    pos_tags = nltk.pos_tag(tokens) #parts of speech identification\n",
        "    ne_tags = nltk.ne_chunk(pos_tags) #entity chuncking\n",
        "    # Initialize empty list to store entities\n",
        "    entities = []\n",
        "    # Iterate over the named entity chunks and finds entity name and labels\n",
        "    for chunk in ne_tags:\n",
        "        if isinstance(chunk, nltk.Tree):\n",
        "            entity = \" \".join([token for token, pos in chunk.leaves()])\n",
        "            label = chunk.label()\n",
        "            if label == 'PERSON' or label == 'ORGANIZATION' or label == 'LOCATION' or label == 'DATE':\n",
        "                entities.append((entity, label))\n",
        "    return entities\n",
        "\n",
        "all_entities = []\n",
        "for index, row in df_cleaned_reviews.iterrows():\n",
        "    review_text = row['Desc']\n",
        "    entities = named_entity_recognition(review_text)\n",
        "    all_entities.extend(entities)\n",
        "\n",
        "# Create DataFrame to store entities and their counts\n",
        "df_entities = pd.DataFrame(all_entities, columns=['Entity', 'Label'])\n",
        "entity_counts = df_entities['Label'].value_counts()\n",
        "\n",
        "# Print counts of each entity type\n",
        "print(\"Entity Counts:\")\n",
        "print(entity_counts)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rwe7AF0L4n1v",
        "outputId": "04986571-c986-4541-98fa-8088539b60cb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity Counts:\n",
            "ORGANIZATION    9020\n",
            "PERSON          8580\n",
            "Name: Label, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ],
      "metadata": {
        "id": "q8BFCvWp32cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# I have learnt how to scrape rewuied data from a website using beautifulsoup\n",
        "# I have learnt how to use the techniques like lemmatization, stemming and stop\n",
        "#words recognition which help me understand how information retireval works\n",
        "# I got to improve my knowlegede on few more information retrival and NLP methoods like POS reconition, named entity"
      ],
      "metadata": {
        "id": "_e557s2w4BpK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "OP4-5qxAtInv"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}