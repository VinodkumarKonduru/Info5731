{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unt-iialab/INFO5731_Spring2020/blob/master/Assignments/INFO5731_Assignment_Four.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Four**\n",
        "\n",
        "In this assignment, you are required to conduct topic modeling, sentiment analysis based on **the dataset you created from assignment three**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1: Topic Modeling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(30 points). This question is designed to help you develop a feel for the way topic modeling works, the connection to the human meanings of documents. Based on the dataset from assignment three, write a python program to **identify the top 10 topics in the dataset**. Before answering this question, please review the materials in lesson 8, especially the code for LDA, LSA, and BERTopic. The following information should be reported:\n",
        "\n",
        "1. Features (text representation) used for topic modeling.\n",
        "\n",
        "2. Top 10 clusters for topic modeling.\n",
        "\n",
        "3. Summarize and describe the topic for each cluster.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuFPKhC0m1fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6709a110-7d9b-4d72-b106-e88005b6e9ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (2.7.0)\n",
            "Requirement already satisfied: bertopic in /usr/local/lib/python3.10/dist-packages (0.16.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.40.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.22.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: hdbscan>=0.8.29 in /usr/local/lib/python3.10/dist-packages (from bertopic) (0.8.33)\n",
            "Requirement already satisfied: umap-learn>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (0.5.6)\n",
            "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (5.15.0)\n",
            "Requirement already satisfied: cython<3,>=0.27 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (0.29.37)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.11.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=4.7.0->bertopic) (8.2.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.0->bertopic) (0.58.1)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.0->bertopic) (0.5.12)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (0.41.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "!pip install pandas scikit-learn gensim sentence-transformers bertopic\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install biterm\n"
      ],
      "metadata": {
        "id": "cgOI0EAZz8U2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5e647b2-efaf-4e1c-e98b-acb9ad79ebad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: biterm in /usr/local/lib/python3.10/dist-packages (0.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biterm) (1.25.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from biterm) (4.66.2)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from biterm) (0.29.37)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from biterm) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->biterm) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->biterm) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->biterm) (2023.12.25)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas gensim nltk\n"
      ],
      "metadata": {
        "id": "bY0rFQAt0C3Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfbc1489-f03b-4e04-b05d-3d54c87b0b64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import gensim.downloader as api\n",
        "\n",
        "# we are downloading the  NLTK stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# we are loading the sentimented dataset from 3rd assignment\n",
        "df = pd.read_csv('amazonreviewssentimented.csv')\n",
        "\n",
        "# we are  preprocessing  the data from the sample file\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "texts = df['clean_text'].apply(lambda x: word_tokenize(str(x).lower()))\n",
        "\n",
        "\n",
        "texts = [[word for word in text if word.isalnum() and word not in stop_words] for text in texts]\n",
        "\n",
        "\n",
        "#we are creating the dictionary representation for the documents\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "\n",
        "# we are creating the bag-of-words\n",
        "corpus = []\n",
        "for t in texts:\n",
        "    bow = dictionary.doc2bow(t)\n",
        "    corpus.append(bow)\n",
        "\n",
        "# we are training the lda model\n",
        "lda_model = LdaModel(corpus, num_topics=10, id2word=dictionary, passes=15)\n",
        "\n",
        "# we are printing the top 10 topics in the database along with their keywords\n",
        "topics = lda_model.print_topics(num_words=5)\n",
        "for tp in topics:\n",
        "    print(tp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "py6Dd9K_0H1C",
        "outputId": "670fb7fc-2966-4e4a-cd0d-3878e63e2c4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0, '0.104*\"customer\" + 0.052*\"apple\" + 0.052*\"included\" + 0.052*\"us\" + 0.052*\"buy\"')\n",
            "(1, '0.002*\"apple\" + 0.002*\"power\" + 0.002*\"devices\" + 0.002*\"watts\" + 0.002*\"charging\"')\n",
            "(2, '0.156*\"works\" + 0.104*\"ipad\" + 0.052*\"something\" + 0.052*\"either\" + 0.052*\"used\"')\n",
            "(3, '0.060*\"charging\" + 0.040*\"adapter\" + 0.033*\"iphone\" + 0.027*\"apple\" + 0.020*\"fast\"')\n",
            "(4, '0.030*\"power\" + 0.029*\"apple\" + 0.022*\"watts\" + 0.019*\"charging\" + 0.016*\"pd\"')\n",
            "(5, '0.028*\"power\" + 0.026*\"apple\" + 0.024*\"watts\" + 0.021*\"charging\" + 0.019*\"pd\"')\n",
            "(6, '0.046*\"charger\" + 0.039*\"charging\" + 0.039*\"apple\" + 0.023*\"speed\" + 0.023*\"20w\"')\n",
            "(7, '0.002*\"charging\" + 0.002*\"adapter\" + 0.002*\"iphone\" + 0.002*\"power\" + 0.002*\"compact\"')\n",
            "(8, '0.050*\"cords\" + 0.050*\"one\" + 0.038*\"connection\" + 0.038*\"cord\" + 0.038*\"charger\"')\n",
            "(9, '0.194*\"great\" + 0.097*\"power\" + 0.097*\"delivery\" + 0.097*\"adapter\" + 0.097*\"fast\"')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2: Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(30 points). Sentiment analysis also known as opinion mining is a sub field within Natural Language Processing (NLP) that builds machine learning algorithms to classify a text according to the sentimental polarities of opinions it contains, e.g., positive, negative, neutral. The purpose of this question is to develop a machine learning classifier for sentiment analysis. Based on the dataset from assignment three, write a python program to implement a sentiment classifier and evaluate its performance. Notice: **80% data for training and 20% data for testing**.  \n",
        "\n",
        "1. Select features for the sentiment classification and explain why you select these features. Use a markdown cell to provide your explanation.\n",
        "\n",
        "2. Select two of the supervised learning algorithms/models from scikit-learn library: https://scikit-learn.org/stable/supervised_learning.html#supervised-learning, to build two sentiment classifiers respectively. Note: Cross-validation (5-fold or 10-fold) should be conducted. Here is the reference of cross-validation: https://scikit-learn.org/stable/modules/cross_validation.html.\n",
        "\n",
        "3. Compare the performance over accuracy, precision, recall, and F1 score for the two algorithms you selected. The test set must be used for model evaluation in this step. Here is the reference of how to calculate these metrics: https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vATjQNTY8buA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7046f681-9a0a-42be-aa72-7a8cd4a5adb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance Metrics for Support Vector Machine:\n",
            "Accuracy: 1.0000\n",
            "Precision: 1.0000\n",
            "Recall: 1.0000\n",
            "F1 Score: 1.0000\n",
            "\n",
            "Performance Metrics for Random Forest:\n",
            "Accuracy: 1.0000\n",
            "Precision: 1.0000\n",
            "Recall: 1.0000\n",
            "F1 Score: 1.0000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# we are loading the sentimented dataset\n",
        "df = pd.read_csv('amazonreviewssentimented.csv')\n",
        "\n",
        "# clean_text\n",
        "x = df['clean_text'].astype(str)\n",
        "\n",
        "# Labels: sentiment\n",
        "y = df['sentiment']\n",
        "\n",
        "# we are separating the dataset into train and test i.e 80% for training and 20% for testing\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# we are getting the Feature using tf_idf\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
        "\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# we are using Support Vector Machine SVM model\n",
        "\n",
        "svm_model = SVC(kernel='linear', C=1)\n",
        "\n",
        "svm_scores = cross_val_score(svm_model, X_train_tfidf, y_train, cv=5, scoring='accuracy')\n",
        "\n",
        "svm_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "svm_predictions = svm_model.predict(X_test_tfidf)\n",
        "\n",
        "\n",
        "# we are using random forest model\n",
        "\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "rf_scores = cross_val_score(rf_model, X_train_tfidf, y_train, cv=5, scoring='accuracy')\n",
        "\n",
        "rf_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "rf_predictions = rf_model.predict(X_test_tfidf)\n",
        "\n",
        "\n",
        "# we are evaluation the  metrics and printing them\n",
        "def evaluate_performance(y_true, y_pred, algorithm_name):\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, average='weighted')\n",
        "    recall = recall_score(y_true, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "    # we are printing the algorithm name\n",
        "    print(f\"Performance Metrics for {algorithm_name}:\")\n",
        "    # we are printing the accuracy\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    # we are printing the precision\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    # wea re printing the recall\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    # we are printing the f1 score\n",
        "    print(f\"F1 Score: {f1:.4f}\\n\")\n",
        "\n",
        "\n",
        "# we are evaluating the  Support Vector Machine performance\n",
        "evaluate_performance(y_test, svm_predictions, 'Support Vector Machine')\n",
        "\n",
        "# we are evaluating the random forest performance\n",
        "evaluate_performance(y_test, rf_predictions, 'Random Forest')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.TF-IDF (Term Frequency-Inverse Document Frequency) Features:\n",
        "this feature represent the most importance of words in reviews relative to a product when we select the top TF-IDF features, this feature focuses on words that are most discriminative for sentiment classification. This feature helps in capturing the semantic meaning of the text and distinguish the difference  between positive and negative sentiments.\n",
        "\n",
        "\n",
        "\n",
        "2.SVM (Support Vector Machine) :this feature is more effective in text classification as it seeks in finding the optimal hyperplane which separates the different classes based on the selected features. It works nice with high-dimensional sparse data like TF-IDF vectors."
      ],
      "metadata": {
        "id": "f-R7wfm3AAdN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3: House price prediction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(20 points). You are required to build a **regression** model to predict the house price with 79 explanatory variables describing (almost) every aspect of residential homes. The purpose of this question is to practice regression analysis, an supervised learning model. The training data, testing data, and data description files can be download from canvas. Here is an axample for implementation: https://towardsdatascience.com/linear-regression-in-python-predict-the-bay-areas-home-price-5c91c8378878.\n",
        "\n",
        "1. Conduct necessary Explatory Data Analysis (EDA) and data cleaning steps on the given dataset. Split data for training and testing.\n",
        "2. Based on the EDA results, select a number of features for the regression model. Shortly explain why you select those features.\n",
        "3. Develop a regression model. The train set should be used.\n",
        "4. Evaluate performance of the regression model you developed using appropriate evaluation metrics. The test set should be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfvMKJjIXS5G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d97b4f0e-6894-4921-92f9-707781285de2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Regression R squared: 0.507109434948416\n",
            "Root Mean Squared Error: 58342.33703683097\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import mean_squared_error\n",
        "# Conduct necessary Explatory Data Analysis (EDA) and data cleaning steps on the given dataset. Split data for training and testing.\n",
        "# we are loading the train and test data\n",
        "trndat = pd.read_csv(\"train.csv\")\n",
        "tstdat = pd.read_csv(\"test.csv\")\n",
        "\n",
        "# we are mixing the both train and test data for preprocessing\n",
        "mixdata = pd.concat([trndat.drop(\"SalePrice\", axis=1), tstdat])\n",
        "\n",
        "# we are separating the number and categorical columns\n",
        "num_clm = mixdata.select_dtypes(include=np.number).columns\n",
        "cat_clm = mixdata.select_dtypes(include='object').columns\n",
        "\n",
        "# we are imputing the missing data for number column\n",
        "numeric_data_imputer = SimpleImputer(strategy='mean')\n",
        "mixdata[num_clm] = numeric_data_imputer.fit_transform(mixdata[num_clm])\n",
        "\n",
        "# we are imputing the missing data for categorical column\n",
        "categorical_data_imputer = SimpleImputer(strategy='most_frequent')\n",
        "\n",
        "mixdata[cat_clm] = categorical_data_imputer.fit_transform(mixdata[cat_clm])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Convert categorical to numerical using label encoder\n",
        "label_encoder = LabelEncoder()\n",
        "for col in cat_clm:\n",
        "    mixdata[col] = label_encoder.fit_transform(mixdata[col])\n",
        "\n",
        "# Splitting back into training and testing data\n",
        "\n",
        "imputed_training_data = mixdata.iloc[:len(trndat)]\n",
        "\n",
        "imputed_testing_data = mixdata.iloc[len(trndat):]\n",
        "\n",
        "# we are scaling the data for better prediction\n",
        "data_scaler = MinMaxScaler()\n",
        "x_tr = imputed_training_data\n",
        "y_train = trndat[\"SalePrice\"]\n",
        "\n",
        "X_train_scaled_min_max = data_scaler.fit_transform(x_tr)\n",
        "X_train_scaled_min_max_df = pd.DataFrame(X_train_scaled_min_max, columns=x_tr.columns)\n",
        "\n",
        "X_test_scaled_min_max = data_scaler.transform(imputed_testing_data)\n",
        "X_test_scaled_min_max_df = pd.DataFrame(X_test_scaled_min_max, columns=imputed_testing_data.columns)\n",
        "\n",
        "# Split data into train and test (80-20)\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_train_scaled_min_max_df, y_train, test_size=0.2, random_state=0)\n",
        "#3.  Developing a regression model. The train set should be used.\n",
        "\n",
        "# Train the Linear Regression model\n",
        "regression_model = LinearRegression()\n",
        "regression_model.fit(x_train, y_train)\n",
        "#4. Evaluating performance of the regression model we developed using appropriate evaluation metrics.using The test set\n",
        "# Predict using the model\n",
        "y_pred = regression_model.predict(x_test)\n",
        "\n",
        "\n",
        "# we are evaluating the model and printing them\n",
        "print('Linear Regression R squared:', regression_model.score(x_test, y_test))\n",
        "m_sqrd_er = mean_squared_error(y_pred, y_test)\n",
        "r_m_sqrd_er = np.sqrt(m_sqrd_er)\n",
        "print('Root Mean Squared Error:', r_m_sqrd_er)\n",
        "\n",
        "# we give Prediction for house prices as per test data\n",
        "pred_price = regression_model.predict(X_test_scaled_min_max_df)\n",
        "\n",
        "# we are Displaying the result\n",
        "tstdat[\"Predicted_SalePrice\"] = pred_price\n",
        "tstdat[['Id', 'Predicted_SalePrice']].to_csv('pred_price.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BbswDvnEX-k"
      },
      "source": [
        "# **Question 4: Using Pre-trained LLMs**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKwKTnW1EX-k"
      },
      "source": [
        "(20 points)\n",
        "Utilize a **Pre-trained Language Model (PLM) from the Hugging Face Repository** for predicting sentiment polarities on the data you collected in Assignment 3.\n",
        "\n",
        "Then, choose a relevant LLM from their repository, such as GPT-3, BERT, or RoBERTa or any other related models.\n",
        "1. (5 points) Provide a brief description of the PLM you selected, including its original pretraining data sources,  number of parameters, and any task-specific fine-tuning if applied.\n",
        "2. (10 points) Use the selected PLM to perform the sentiment analysis on the data collected in Assignment 3. Only use the model in the **zero-shot** setting, NO finetuning is required. Evaluate performance of the model by comparing with the groundtruths (labels you annotated) on Accuracy, Precision, Recall, and F1 metrics.\n",
        "3. (5 points) Discuss the advantages and disadvantages of the selected PLM, and any challenges encountered during the implementation. This will enable a comprehensive understanding of the chosen LLM's applicability and effectiveness for the given task.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the libraries\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from transformers import TFBertForSequenceClassification, BertTokenizer\n",
        "from transformers import pipeline\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# we are reading the sentimented file\n",
        "sample = pd.read_csv('amazonreviewssentimented.csv')\n",
        "# we are inputting cleantext to list\n",
        "t = sample['clean_text'].tolist()\n",
        "# we are adding sentiments of the reviews into a list\n",
        "l = sample['sentiment'].tolist()\n",
        "# we are using tokinizer for pre-training the model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "# we are anlalysing the sentiments using pipeline\n",
        "sentiment_pipeline = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer, framework='tf')\n",
        "\n",
        "pred = []\n",
        "for tx in t:\n",
        "    tokenized_inputs = tokenizer(tx, truncation=True, max_length=512, return_tensors='tf')\n",
        "\n",
        "    out = model(tokenized_inputs['input_ids'], tokenized_inputs['attention_mask'])\n",
        "\n",
        "    logits = out.logits\n",
        "    predicted_label_index = tf.argmax(logits, axis=1).numpy()[0]\n",
        "    # we are checcking the sentiments in if cases\n",
        "    if predicted_label_index == 0:\n",
        "        pred.append('negative')\n",
        "    elif predicted_label_index == 1:\n",
        "        pred.append('neutral')\n",
        "    else:\n",
        "        pred.append('positive')\n",
        "\n",
        "accuracy = accuracy_score(l, pred)\n",
        "precision = precision_score(l, pred, average='weighted')\n",
        "recall = recall_score(l, pred, average='weighted')\n",
        "f1 = f1_score(l, pred, average='weighted')\n",
        "# we are printing the accuracy\n",
        "print(f'Accuracy: {accuracy:.2f}')\n",
        "# we are printing the precission\n",
        "print(f'Precision: {precision:.2f}')\n",
        "# we are printing the recall\n",
        "print(f'Recall: {recall:.2f}')\n",
        "# we are printing the f1 scores\n",
        "print(f'F1-score: {f1:.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGcv0rV0xelR",
        "outputId": "0067c19a-75ae-4393-9bc0-9fd6023fe605"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.10\n",
            "Precision: 0.01\n",
            "Recall: 0.10\n",
            "F1-score: 0.02\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WOxyuXEa05oZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-trained Language Model (PLM):\n",
        "we choose DistilBERT model for Hugging Face Repository DistilBERT is a distilled version of the  (Bidirectional Encoder Representations from Transformers) model.\n",
        "\n",
        "Original Pretraining Data Sources: BooksCorpus (800M words) and the English Wikipedia (13GB) are two of the data sources used to pretrain DistilBERT. On the other hand, DistilBERT compresses the original BERT model using knowledge distillation techniques, leading to a more compact and effective design.\n",
        "\n",
        "Parameter count: DistilBERT contains about 66 million parameters, a substantial decrease from the 110 million parameters in the BERT base version. DistilBERT is now more lightweight and appropriate for deployment in contexts with limited resources because to this reduction in parameters.\n",
        "\n",
        "Advantages of DistilBERT:\n",
        "\n",
        "Efficiency:\n",
        "in this we has few parameters when compared to bert.\n",
        "\n",
        "Speed:\n",
        "as it only uses only few parameters so it provides more speed.\n",
        "\n",
        "Disadvantages of DistilBERT:\n",
        "\n",
        "Limited Contextual Understanding:\n",
        "when we compared with bert it has very limited understanding of language due to reduced capacity in it.\n",
        "\n",
        "Less Fine-grained Representations:\n",
        "DistilBERT  will results in less fine-grained representations of language\n",
        "because of little understanding it effects the performance.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FYkTwPgT06HT"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}