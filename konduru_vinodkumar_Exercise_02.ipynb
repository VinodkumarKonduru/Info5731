{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DymRJbxDBCnf"
   },
   "source": [
    "# **INFO5731 In-class Exercise 2**\n",
    "\n",
    "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
    "\n",
    "**Expectations**:\n",
    "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
    "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
    "*   Write complete answers and run all the cells before submission.\n",
    "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
    "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
    "\n",
    "**Total points**: 40\n",
    "\n",
    "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
    "\n",
    "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBKvD6O_TY6e"
   },
   "source": [
    "## Question 1 (10 Points)\n",
    "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "cikVKDXdTbzE",
    "outputId": "582d0bb7-b0d8-40cf-95f0-780c53265d69"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Job_Satisfaction\\nProductivity\\nAge\\nYears_of_Experience'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write your answer here\n",
    "# The implementation of artificial intelligence in an organisation.\n",
    "# We need combination of qualitative and quantitative data containing employee's\n",
    "Job_Satisfaction\n",
    "Productivity\n",
    "Age\n",
    "Years_of_Experience\n",
    "\n",
    "# we are collecting 1000 samples for analysis.\n",
    "#initially we are generating job satisfaction scores and then employee productivity scores\n",
    "#we then generate employee age and experience and then we create a data frames for the titles\n",
    "#we then store the collected data into a csv file using data to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9RqrlwdTfvl"
   },
   "source": [
    "## Question 2 (10 Points)\n",
    "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4XvRknixTh1g",
    "outputId": "54c96202-2bdd-4a99-fcf9-07d651a670b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Job_Satisfaction  Productivity  Age  Years_of_Experience\n",
      "0                 1            15   52                    2\n",
      "1                 3             2   39                   17\n",
      "2                 1            70   39                    8\n",
      "3                 3            54   57                   15\n",
      "4                 3             1   23                   29\n"
     ]
    }
   ],
   "source": [
    "# write your answer here\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# total samples\n",
    "num_smpl = 1000\n",
    "\n",
    "# Generating employee job satisfaction scores\n",
    "job_sat_score = np.random.randint(1, 6, size=num_smpl)\n",
    "\n",
    "# Generating employee productivity measures from 0 to 100\n",
    "prod_score = np.random.randint(0, 101, size=num_smpl)\n",
    "\n",
    "# Generating  demographic information i.e age and experience\n",
    "age = np.random.randint(22, 60, size=num_smpl)\n",
    "year_of_exp = np.random.randint(1, 31, size=num_smpl)\n",
    "\n",
    "# Creating DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'Job_Satisfaction': job_sat_score,\n",
    "    'Productivity': prod_score,\n",
    "    'Age': age,\n",
    "    'Years_of_Experience': year_of_exp\n",
    "})\n",
    "\n",
    "# storing data in a CSV file\n",
    "data.to_csv('remote_work_impact_dataset.csv', index=False)\n",
    "\n",
    "# printing data  from dataset\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03jb4GZsBkBS"
   },
   "source": [
    "## Question 3 (10 Points)\n",
    "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
    "\n",
    "The following information from the article needs to be collected:\n",
    "\n",
    "(1) Title of the article\n",
    "\n",
    "(2) Venue/journal/conference being published\n",
    "\n",
    "(3) Year\n",
    "\n",
    "(4) Authors\n",
    "\n",
    "(5) Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YaGLbSHHB8Ej",
    "outputId": "9d29238d-1db5-4016-902a-595a933883a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scholarly in c:\\users\\kalya\\anaconda3\\lib\\site-packages (1.7.11)\n",
      "Requirement already satisfied: arrow in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from scholarly) (1.2.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from scholarly) (4.12.2)\n",
      "Requirement already satisfied: bibtexparser in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from scholarly) (1.4.1)\n",
      "Requirement already satisfied: deprecated in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from scholarly) (1.2.14)\n",
      "Requirement already satisfied: fake-useragent in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from scholarly) (1.4.0)\n",
      "Requirement already satisfied: free-proxy in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from scholarly) (1.1.1)\n",
      "Requirement already satisfied: httpx in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from scholarly) (0.26.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from scholarly) (0.21.0)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from scholarly) (2.31.0)\n",
      "Requirement already satisfied: selenium in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from scholarly) (4.17.2)\n",
      "Requirement already satisfied: sphinx-rtd-theme in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from scholarly) (2.0.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from scholarly) (4.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.0 in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from arrow->scholarly) (2.8.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from beautifulsoup4->scholarly) (2.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.3 in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from bibtexparser->scholarly) (3.0.9)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from deprecated->scholarly) (1.14.1)\n",
      "Requirement already satisfied: lxml in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from free-proxy->scholarly) (4.9.3)\n",
      "Requirement already satisfied: anyio in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from httpx->scholarly) (3.5.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from httpx->scholarly) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from httpx->scholarly) (1.0.3)\n",
      "Requirement already satisfied: idna in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from httpx->scholarly) (3.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from httpx->scholarly) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx->scholarly) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from requests[socks]->scholarly) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from requests[socks]->scholarly) (1.26.16)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from requests[socks]->scholarly) (1.7.1)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from selenium->scholarly) (0.24.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from selenium->scholarly) (0.11.1)\n",
      "Requirement already satisfied: sphinx<8,>=5 in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from sphinx-rtd-theme->scholarly) (5.0.2)\n",
      "Requirement already satisfied: docutils<0.21 in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from sphinx-rtd-theme->scholarly) (0.18.1)\n",
      "Requirement already satisfied: sphinxcontrib-jquery<5,>=4 in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from sphinx-rtd-theme->scholarly) (4.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.0->arrow->scholarly) (1.16.0)\n",
      "Requirement already satisfied: sphinxcontrib-applehelp in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (1.0.2)\n",
      "Requirement already satisfied: sphinxcontrib-devhelp in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (1.0.2)\n",
      "Requirement already satisfied: sphinxcontrib-jsmath in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (1.0.1)\n",
      "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (2.0.0)\n",
      "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (1.1.5)\n",
      "Requirement already satisfied: sphinxcontrib-qthelp in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (1.0.3)\n",
      "Requirement already satisfied: Jinja2>=2.3 in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (3.1.2)\n",
      "Requirement already satisfied: Pygments>=2.0 in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (2.15.1)\n",
      "Requirement already satisfied: snowballstemmer>=1.1 in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (2.2.0)\n",
      "Requirement already satisfied: babel>=1.3 in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (2.11.0)\n",
      "Requirement already satisfied: alabaster<0.8,>=0.7 in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (0.7.12)\n",
      "Requirement already satisfied: imagesize in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (1.4.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (23.1)\n",
      "Requirement already satisfied: colorama>=0.3.5 in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (0.4.6)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium->scholarly) (22.1.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium->scholarly) (2.4.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium->scholarly) (1.3.0.post0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium->scholarly) (1.15.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium->scholarly) (1.2.0)\n",
      "Requirement already satisfied: pytz>=2015.7 in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from babel>=1.3->sphinx<8,>=5->sphinx-rtd-theme->scholarly) (2023.3.post1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium->scholarly) (2.21)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from Jinja2>=2.3->sphinx<8,>=5->sphinx-rtd-theme->scholarly) (2.1.1)\n"
     ]
    }
   ],
   "source": [
    "# write your answer here\n",
    "!pip install scholarly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mb0dlAKMuGfO",
    "outputId": "ce68b019-bc92-4d11-ef0e-476f226c7163"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred: Cannot Fetch from Google Scholar.\n",
      "Retrying after 2 seconds...\n",
      "Error occurred: Cannot Fetch from Google Scholar.\n",
      "Retrying after 2 seconds...\n",
      "Error occurred: Cannot Fetch from Google Scholar.\n",
      "Max retries exceeded. Exiting.\n"
     ]
    }
   ],
   "source": [
    "from scholarly import scholarly\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "def collect_articles(query, num_articles):\n",
    "    articles = []\n",
    "    current_year = datetime.datetime.now().year\n",
    "    max_tries = 3\n",
    "    tries = 0\n",
    "\n",
    "    while tries < max_tries:\n",
    "        try:\n",
    "            for article in scholarly.search_pubs(query):\n",
    "\n",
    "                if len(articles) >= num_articles:\n",
    "                    break\n",
    "                articles.append({\n",
    "                        \"title\": article[\"bib\"][\"title\"],\n",
    "                        \"venue\": article[\"bib\"][\"journal\"],\n",
    "                        \"year\": article[\"bib\"][\"pub_year\"],\n",
    "                        \"authors\": article[\"bib\"]['author'],\n",
    "                        \"abstract\": article[\"bib\"]['abstract']\n",
    "                    })\n",
    "\n",
    "            return articles\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred: {e}\")\n",
    "            tries += 1\n",
    "            if tries < max_tries:\n",
    "                print(\"Retrying after 2 seconds...\")\n",
    "                time.sleep(2)\n",
    "            else:\n",
    "                print(\"Max retries exceeded. Exiting.\")\n",
    "\n",
    "    return articles\n",
    "\n",
    "query = \"XYZ\"\n",
    "num_articles = 10\n",
    "articles = collect_articles(query, num_articles)\n",
    "\n",
    "# Print the collected articles\n",
    "for i, article in enumerate(articles, 1):\n",
    "    print(f\"Article {i}:\")\n",
    "    print(\"Title:\", article['title'])\n",
    "    print(\"Venue:\", article['venue'])\n",
    "    print(\"Year:\", article['year'])\n",
    "    print(\"Authors:\", article['authors'])\n",
    "    print(\"Abstract:\", article['abstract'])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJDe71iLB616"
   },
   "source": [
    "## Question 4A (10 Points)\n",
    "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
    "\n",
    "\n",
    "\n",
    "Ensure that the collected data has more than four columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MtKskTzbCLaU",
    "outputId": "bf95de20-f861-4df7-bdf0-7aa6d8ee1cb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting instaloader\n",
      "  Downloading instaloader-4.10.3.tar.gz (62 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/62.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m51.2/62.9 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.9/62.9 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: requests>=2.4 in /usr/local/lib/python3.10/dist-packages (from instaloader) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.4->instaloader) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.4->instaloader) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.4->instaloader) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.4->instaloader) (2024.2.2)\n",
      "Building wheels for collected packages: instaloader\n",
      "  Building wheel for instaloader (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for instaloader: filename=instaloader-4.10.3-py3-none-any.whl size=64767 sha256=79c38113f71a1b825103a1f19c0bf5594041cbe8db491db1b21af7f0981021f7\n",
      "  Stored in directory: /root/.cache/pip/wheels/79/98/b7/4c15fe6680cf0e460b20fba742cf5052461e8b320f6f9f7e21\n",
      "Successfully built instaloader\n",
      "Installing collected packages: instaloader\n",
      "Successfully installed instaloader-4.10.3\n"
     ]
    }
   ],
   "source": [
    "# write your answer here\n",
    "!pip install instaloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 616
    },
    "id": "Lhjs4Z6YurhC",
    "outputId": "38096a39-fa99-43d1-91c3-c9886640f88e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred: argument of type 'builtin_function_or_method' is not iterable\n",
      "None\n",
      "Failed to collect Instagram data.\n"
     ]
    }
   ],
   "source": [
    "from instaloader import Instaloader, Profile\n",
    "\n",
    "def collect_instagram_data(username, hashtag, num_posts):\n",
    "    L = Instaloader()\n",
    "    posts_data = []\n",
    "    \n",
    "    try:\n",
    "        profile = Profile.from_username(L.context, username)\n",
    "        \n",
    "        for post in profile.get_posts():\n",
    "            if hashtag.lower() in post.caption.lower:\n",
    "                posts_data.append({\n",
    "                    \"shortcode\": post.shortcode,\n",
    "                    \"url\": f\"https://www.instagram.com/p/{post.shortcode}/\",\n",
    "                    \"caption\": post.caption,\n",
    "                    \"likes\": post.likes,\n",
    "                    \"comments\": post.comments,\n",
    "                    \"timestamp\": post.date_utc,\n",
    "                    \"owner_username\": post.owner_username,\n",
    "                    \"owner_id\": post.owner_id,\n",
    "                    \"owner_followers\": post.owner_profile.followers,\n",
    "                    \"owner_following\": post.owner_profile.followees,\n",
    "                })\n",
    "\n",
    "                if len(posts_data) >= num_posts:\n",
    "                    break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "    return posts_data\n",
    "\n",
    "\n",
    "username = \"chennaiipl\"  \n",
    "hashtag = \"SuperBirthday\"  \n",
    "num_posts = 1\n",
    "instagram_data = collect_instagram_data(username, hashtag, num_posts)\n",
    "\n",
    "print(instagram_data)\n",
    "\n",
    "if instagram_data is not None:\n",
    "    for post in instagram_data:\n",
    "        print(post)\n",
    "else:\n",
    "    print(\"Failed to collect Instagram data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n3XI5Wfq9GOc"
   },
   "outputs": [],
   "source": [
    "from instaloader import Instaloader, Hashtag  # Import the Hashtag class\n",
    "\n",
    "def collect_posts(hashtag, num_posts):\n",
    "    L = Instaloader()\n",
    "    posts = []\n",
    "    hashtag_obj = Hashtag.from_name(L.context, hashtag)  # Create a Hashtag object\n",
    "\n",
    "    # Rest of your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55W9AMdXCSpV"
   },
   "source": [
    "## Question 4B (10 Points)\n",
    "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
    "\n",
    "\n",
    "\n",
    "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
    "\n",
    "Please only choose one option for question 4. If you do both options, we will grade only the first one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I57NXsauCec2"
   },
   "outputs": [],
   "source": [
    "# write your answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZOhks1dXWEe"
   },
   "source": [
    "# Mandatory Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqmHVEwaWhbV"
   },
   "source": [
    "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
    "\n",
    "\n",
    "\n",
    "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
    "\n",
    "\n",
    "\n",
    "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
    "\n",
    "\n",
    "\n",
    "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
    "\n",
    "\n",
    "\n",
    "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
    "\n",
    "**(no grading of your submission if this question is left unanswered)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "akAVJn9YBTQT"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Write your response here.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "FBKvD6O_TY6e",
    "E9RqrlwdTfvl",
    "03jb4GZsBkBS",
    "jJDe71iLB616",
    "55W9AMdXCSpV",
    "4ulBZ6yhCi9F",
    "6SmvS7nSfbj8",
    "sZOhks1dXWEe"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
