{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 3**\n",
        "\n",
        "The purpose of this exercise is to explore various aspects of text analysis, including feature extraction, feature selection, and text similarity ranking.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting text classification or text mining task and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "VAZj4PHB70nf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "c091641f-5e53-4257-a93b-57816f044440"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nPlease write you answer here:\\nwe are going to perform sentimental analysis on a movie based on the viewers reviews\\n1:N-grams:N-grams are set of 'n' words or characters from a text. it helps in finding how people feeling about some thing.\\n2:TF-IDF: TF-IDF features show how important words are, it help by highlighting the words\\n3:Bag of words (BOW): we are counting the words often repeating in the reviews\\n4:Part-of-Speech (POS):It label the different types of speech like nouns, adjectives, and verbs in the text.\\n5:Sentiment Lexicons:It helps in analyze the feelings expressed by humans on somw content.\\n\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 100
        }
      ],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "we are going to perform sentimental analysis on a movie based on the viewers reviews\n",
        "1:N-grams:N-grams are set of 'n' words or characters from a text. it helps in finding how people feeling about some thing.\n",
        "2:TF-IDF: TF-IDF features show how important words are, it help by highlighting the words\n",
        "3:Bag of words (BOW): we are counting the words often repeating in the reviews\n",
        "4:Part-of-Speech (POS):It label the different types of speech like nouns, adjectives, and verbs in the text.\n",
        "5:Sentiment Lexicons:It helps in analyze the feelings expressed by humans on somw content.\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b84634cb-59a0-4778-97a5-abae24683278"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "# we are importing all the required packages\n",
        "import nltk\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import spacy\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "# we are downloading  the required packages\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "revieews = [\"The movie is awesome.\",\"The movie is  flop. \",\"The movie is  good. \",]\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nltk.download(\"vader_lexicon\")\n",
        "\n",
        "S = SentimentIntensityAnalyzer()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# n-grams\n",
        "ng_v = CountVectorizer(ngram_range=(1, 2), stop_words=stopwords.words(\"english\"))\n",
        "ng_f = ng_v.fit_transform(revieews)\n",
        "ng_df = pd.DataFrame(ng_f.toarray(), columns=ng_v.get_feature_names_out())\n",
        "print(\"we are printing\")\n",
        "# we are printing n-grams\n",
        "print(\"\\nN-Grams:\\n\", ng_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4SFerqzU215",
        "outputId": "2e5fca2e-4315-4805-df16-4d13f122a37e"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "we are printing\n",
            "\n",
            "N-Grams:\n",
            "    awesome  flop  good  movie  movie awesome  movie flop  movie good\n",
            "0        1     0     0      1              1           0           0\n",
            "1        0     1     0      1              0           1           0\n",
            "2        0     0     1      1              0           0           1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Term Frequency-Inverse Document Frequency (TF-IDF)\n",
        "tf_v = TfidfVectorizer(stop_words=stopwords.words(\"english\"))\n",
        "tf_f = tf_v.fit_transform(revieews )\n",
        "tf_df = pd.DataFrame(tf_f.toarray(), columns=tf_v.get_feature_names_out())\n",
        "print(\"we are printing\")\n",
        "# we are printing Term Frequency-Inverse Document Frequency\n",
        "print(\"\\nTF-IDF:\\n\", tf_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLAdTxsgYBq5",
        "outputId": "5c34d25d-d0be-4ba4-fc64-8de71aef9bf4"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "we are printing\n",
            "\n",
            "TF-IDF:\n",
            "     awesome      flop      good     movie\n",
            "0  0.861037  0.000000  0.000000  0.508542\n",
            "1  0.000000  0.861037  0.000000  0.508542\n",
            "2  0.000000  0.000000  0.861037  0.508542\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bag of Words (BoW)\n",
        "cv = CountVectorizer(stop_words=stopwords.words(\"english\"))\n",
        "Bow_f = cv.fit_transform(revieews)\n",
        "Bow_df = pd.DataFrame(Bow_f.toarray(), columns=cv.get_feature_names_out())\n",
        "print(\"we are printing\")\n",
        "# we are printing bags of words\n",
        "print(\"Bag of Words (BoW):\\n\", Bow_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhdmHS0WVAIq",
        "outputId": "08e190c2-9f51-44a2-aaa0-25ccc685ee6f"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "we are printing\n",
            "Bag of Words (BoW):\n",
            "    awesome  flop  good  movie\n",
            "0        1     0     0      1\n",
            "1        0     1     0      1\n",
            "2        0     0     1      1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part-of-Speech (POS) Tags\n",
        "tag = [nltk.pos_tag(nltk.word_tokenize(text)) for text in revieews]\n",
        "print(\"we are printing\")\n",
        "# we are printing the parts of the speach\n",
        "print(\"\\nPart-of-Speech :\\n\", tag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlZzeZuKVE_c",
        "outputId": "29f13da4-f3e6-4949-a4ed-c6e8a736fe9d"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "we are printing\n",
            "\n",
            "Part-of-Speech :\n",
            " [[('The', 'DT'), ('movie', 'NN'), ('is', 'VBZ'), ('awesome', 'JJ'), ('.', '.')], [('The', 'DT'), ('movie', 'NN'), ('is', 'VBZ'), ('flop', 'VBN'), ('.', '.')], [('The', 'DT'), ('movie', 'NN'), ('is', 'VBZ'), ('good', 'JJ'), ('.', '.')]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment Lexicons (VADER sentiment scores)\n",
        "snt_scr = []\n",
        "\n",
        "# we are Iterate over the review in the list\n",
        "for text in revieews:\n",
        "    scores = S.polarity_scores(text)\n",
        "    # we are appending the list\n",
        "    snt_scr.append(scores)\n",
        "\n",
        "\n",
        "sent_df = pd.DataFrame(snt_scr)\n",
        "print(\"we are printing\")\n",
        "# we are printing the sentiment lexicons\n",
        "print(\"\\nSentiment Lexicons (VADER):\\n\", sent_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmVZxQuIWeqr",
        "outputId": "b85f8f1d-ed51-45d7-90ef-f7177749a320"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "we are printing\n",
            "\n",
            "Sentiment Lexicons (VADER):\n",
            "      neg    neu    pos  compound\n",
            "0  0.000  0.423  0.577    0.6249\n",
            "1  0.444  0.556  0.000   -0.3400\n",
            "2  0.000  0.508  0.492    0.4404\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "## Question 3 (10 points):\n",
        "Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\"\n",
        "\n",
        "Select the most important features you extracted above, rank the features based on their importance in the descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "2CRuXfV570ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a8a7915-bd0c-488a-990a-6331df2dd544"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature: awesome, Importance: 0.3824\n",
            "Feature: flop, Importance: 0.3176\n",
            "Feature: good, Importance: 0.3000\n",
            "Feature: is, Importance: 0.0000\n",
            "Feature: movie, Importance: 0.0000\n",
            "Feature: the, Importance: 0.0000\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "# we are assining the lanels\n",
        "labels = ['fabulous', 'disaster', 'average']\n",
        "\n",
        "label_dic = {label: idx for idx, label in enumerate(labels)}\n",
        "numlabels = [label_dic[label] for label in labels]\n",
        "\n",
        "# we are creating vectorizer\n",
        "tf_vect = TfidfVectorizer()\n",
        "tf_feat = tf_vect.fit_transform(revieews)\n",
        "\n",
        "# we arre training classifiers\n",
        "rfrst_cf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rfrst_cf.fit(tf_feat, numlabels)\n",
        "\n",
        "f_imp = rfrst_cf.feature_importances_\n",
        "\n",
        "fimp_df = pd.DataFrame({'Feature': tf_vect.get_feature_names_out(), 'Importance': f_imp})\n",
        "# we are sorting the values in decending order\n",
        "sort_feat = fimp_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "n = 10\n",
        "# we are iterating the head for 10 times\n",
        "for idx, row in sort_feat.head(n).iterrows():\n",
        "    print(f\"Feature: {row['Feature']}, Importance: {row['Importance']:.4f}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EI_SgNS1Vdbo",
        "outputId": "745f773b-c17b-4141-eb5a-0469dd01037b"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "## Question 4 (10 points):\n",
        "Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "b4HoWK-i70ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cd31383-9b1d-4c34-9c2c-18d897fe656e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rankings are:\n",
            "Rank 1: sim = 0.7411\n",
            "Text: The movie is  good. \n",
            "\n",
            "Rank 2: sim = 0.6943\n",
            "Text: The movie is awesome.\n",
            "\n",
            "Rank 3: sim = 0.6569\n",
            "Text: The movie is  flop. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "# we are assing the question\n",
        "question = \"how is the movie\"\n",
        "\n",
        "modelname = \"bert-base-uncased\"\n",
        "token = AutoTokenizer.from_pretrained(modelname)\n",
        "# wee are calling the pretrained function\n",
        "model = AutoModel.from_pretrained(modelname)\n",
        "# we are creating the function for bert embeeding\n",
        "def get_bert_embd(text):\n",
        "    inp = token(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    with torch.no_grad():\n",
        "        outputs_tokens = model(**inp)\n",
        "    op_embeddings = outputs_tokens.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
        "    return op_embeddings\n",
        "\n",
        "# we are calling the bert embeeding function\n",
        "query_embed = get_bert_embd(question)\n",
        "review_embed= [get_bert_embd(text) for text in revieews]\n",
        "\n",
        "similar = cosine_similarity([query_embed ], review_embed)[0]\n",
        "# we are sorting the ranks using lambda function\n",
        "rankinngs = sorted(enumerate(similar ), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# we are printing the ranks\n",
        "print(\"Rankings are:\")\n",
        "\n",
        "for i, (index, sim) in enumerate(rankinngs):\n",
        "    print(f\"Rank {i + 1}: sim = {sim:.4f}\")\n",
        "    print(f\"Text: {revieews[index]}\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on extracting features from text data. What were the key concepts or techniques you found most beneficial in understanding the process?\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "i feel this assignments helps a lot in gaining the knowledgs.\n",
        "i felt it is a bit difficult to complete it in short time.\n",
        "me as a data science students i find this exercise added more knowledge to me while working on it.\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "CAq0DZWAhU9m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "52638042-84f0-40d1-b751-0e8b82258f45"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nPlease write you answer here:\\ni feel this assignments helps a lot in gaining the knowledgs.\\ni felt it is a bit difficult to complete it in short time.\\nme as a data science students i find this exercise added more knowledge to me while working on it.\\n\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}